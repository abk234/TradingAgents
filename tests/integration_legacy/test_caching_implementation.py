# Copyright (c) 2024. All rights reserved.
# Licensed under the Apache License, Version 2.0. See LICENSE file in the project root for license information.

"""
Test script for price caching and LLM tracking implementation.
Tests:
1. Price cache operations (store, retrieve, invalidate)
2. Price cache integration with route_to_vendor
3. LLM tracking in analysis storage
4. Cache cleanup functionality
"""

import logging
from datetime import date, timedelta, datetime
import time

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def test_price_cache_operations():
    """Test basic price cache CRUD operations."""
    logger.info("\n" + "="*80)
    logger.info("TEST 1: Price Cache Operations")
    logger.info("="*80)

    from tradingagents.database import get_db_connection
    from tradingagents.database.price_cache_ops import PriceCacheOperations
    from tradingagents.database.ticker_ops import TickerOperations

    try:
        db = get_db_connection()
        cache_ops = PriceCacheOperations(db)
        ticker_ops = TickerOperations(db)

        # Ensure AAPL ticker exists
        ticker_id = ticker_ops.get_or_create_ticker(
            symbol="AAPL",
            company_name="Apple Inc.",
            sector="Technology",
            industry="Consumer Electronics"
        )

        # Create sample price data
        end_date = date.today()
        start_date = end_date - timedelta(days=5)

        sample_prices = []
        current_date = start_date
        price = 150.0

        while current_date <= end_date:
            # Skip weekends
            if current_date.weekday() < 5:
                sample_prices.append({
                    'date': current_date,
                    'open': price,
                    'high': price + 2.0,
                    'low': price - 1.5,
                    'close': price + 1.0,
                    'adj_close': price + 1.0,
                    'volume': 50000000
                })
                price += 1.0
            current_date += timedelta(days=1)

        # Test storing prices
        logger.info(f"Storing {len(sample_prices)} prices for AAPL...")
        count = cache_ops.store_prices(
            ticker_symbol="AAPL",
            prices=sample_prices,
            data_source="test_yfinance",
            is_realtime=False
        )
        assert count == len(sample_prices), f"Expected to store {len(sample_prices)} prices, stored {count}"
        logger.info(f"‚úì Successfully stored {count} prices")

        # Test retrieving prices
        logger.info(f"Retrieving cached prices for AAPL from {start_date} to {end_date}...")
        cached_prices = cache_ops.get_cached_prices("AAPL", start_date, end_date)
        assert cached_prices is not None, "Cache should return prices"
        assert len(cached_prices) == len(sample_prices), f"Expected {len(sample_prices)} prices, got {len(cached_prices)}"
        logger.info(f"‚úì Successfully retrieved {len(cached_prices)} prices from cache")

        # Test cache stats
        stats = cache_ops.get_cache_stats()
        logger.info(f"‚úì Cache stats: {stats}")

        logger.info("‚úÖ TEST 1 PASSED: Price Cache Operations")
        return True

    except Exception as e:
        logger.error(f"‚ùå TEST 1 FAILED: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_cache_integration():
    """Test price caching integration with route_to_vendor."""
    logger.info("\n" + "="*80)
    logger.info("TEST 2: Price Caching Integration with route_to_vendor")
    logger.info("="*80)

    try:
        from tradingagents.dataflows.interface import route_to_vendor_with_cache
        from datetime import date, timedelta

        end_date = date.today()
        start_date = end_date - timedelta(days=10)

        # First call - should fetch from vendor and cache
        logger.info("First call (cache miss expected)...")
        start_time = time.time()
        data1 = route_to_vendor_with_cache("get_stock_data", "AAPL", start_date.strftime("%Y-%m-%d"), end_date.strftime("%Y-%m-%d"))
        time1 = time.time() - start_time
        logger.info(f"‚úì First call completed in {time1:.2f} seconds, data size: {len(data1)} chars")

        # Second call - should hit cache
        logger.info("Second call (cache hit expected)...")
        start_time = time.time()
        data2 = route_to_vendor_with_cache("get_stock_data", "AAPL", start_date.strftime("%Y-%m-%d"), end_date.strftime("%Y-%m-%d"))
        time2 = time.time() - start_time
        logger.info(f"‚úì Second call completed in {time2:.2f} seconds, data size: {len(data2)} chars")

        # Verify cache hit was faster
        if time2 < time1:
            speedup = time1 / time2 if time2 > 0 else float('inf')
            logger.info(f"‚úì Cache hit was {speedup:.1f}x faster!")
        else:
            logger.warning(f"‚ö† Cache hit was not faster (may be first run)")

        # Verify data is identical
        assert data1 == data2, "Cached data should match original data"
        logger.info("‚úì Data consistency verified")

        logger.info("‚úÖ TEST 2 PASSED: Price Caching Integration")
        return True

    except Exception as e:
        logger.error(f"‚ùå TEST 2 FAILED: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_llm_tracking():
    """Test LLM tracking in analysis storage."""
    logger.info("\n" + "="*80)
    logger.info("TEST 3: LLM Tracking in Analysis Storage")
    logger.info("="*80)

    try:
        from tradingagents.database import get_db_connection
        from tradingagents.database.analysis_ops import AnalysisOperations
        from tradingagents.database.ticker_ops import TickerOperations

        db = get_db_connection()
        analysis_ops = AnalysisOperations(db)
        ticker_ops = TickerOperations(db)

        # Get/create ticker
        ticker_id = ticker_ops.get_or_create_ticker(
            symbol="NVDA",
            company_name="NVIDIA Corporation",
            sector="Technology",
            industry="Semiconductors"
        )

        # Create sample analysis data
        analysis_data = {
            'price': 500.25,
            'volume': 30000000,
            'final_decision': 'BUY',
            'confidence_score': 0.85,
            'executive_summary': 'Test analysis with LLM tracking',
            'full_report': {'test': 'data'}
        }

        # Create sample LLM tracking data
        llm_prompts = {
            'fundamentals_analyst': 'Analyze the fundamental data for NVDA...',
            'technical_analyst': 'Review technical indicators for NVDA...',
            'trader': 'Make a trading decision based on analysis...'
        }

        llm_responses = {
            'fundamentals_analyst': {
                'report': 'Strong fundamentals...',
                'score': 85
            },
            'technical_analyst': {
                'report': 'Bullish technical setup...',
                'score': 88
            },
            'trader': {
                'action': 'BUY',
                'confidence': 0.85
            }
        }

        llm_metadata = {
            'model': 'gpt-4o',
            'total_tokens': 12000,
            'prompt_tokens': 7000,
            'completion_tokens': 5000,
            'estimated_cost_usd': 0.15,
            'total_duration_seconds': 8.5
        }

        # Store analysis with LLM tracking
        logger.info("Storing analysis with LLM tracking data...")
        analysis_id = analysis_ops.store_analysis(
            ticker_id=ticker_id,
            analysis_data=analysis_data,
            llm_prompts=llm_prompts,
            llm_responses=llm_responses,
            llm_metadata=llm_metadata
        )

        logger.info(f"‚úì Stored analysis {analysis_id} with LLM tracking")

        # Retrieve and verify
        logger.info("Retrieving analysis to verify LLM data...")
        analyses = analysis_ops.get_analyses_for_ticker(ticker_id, limit=1)
        assert len(analyses) > 0, "Should have at least one analysis"

        latest = analyses[0]
        logger.info(f"‚úì Retrieved analysis: {latest.get('analysis_id')}")

        # Check if LLM fields exist and have data
        has_prompts = latest.get('llm_prompts') is not None
        has_responses = latest.get('llm_responses') is not None
        has_metadata = latest.get('llm_metadata') is not None

        logger.info(f"  - LLM prompts stored: {has_prompts}")
        logger.info(f"  - LLM responses stored: {has_responses}")
        logger.info(f"  - LLM metadata stored: {has_metadata}")

        assert has_prompts, "LLM prompts should be stored"
        assert has_responses, "LLM responses should be stored"
        assert has_metadata, "LLM metadata should be stored"

        logger.info("‚úÖ TEST 3 PASSED: LLM Tracking")
        return True

    except Exception as e:
        logger.error(f"‚ùå TEST 3 FAILED: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_cache_cleanup():
    """Test cache cleanup functionality."""
    logger.info("\n" + "="*80)
    logger.info("TEST 4: Cache Cleanup")
    logger.info("="*80)

    try:
        from tradingagents.database import get_db_connection
        from tradingagents.database.price_cache_ops import PriceCacheOperations

        db = get_db_connection()
        cache_ops = PriceCacheOperations(db)

        # Get stats before cleanup
        stats_before = cache_ops.get_cache_stats()
        logger.info(f"Cache stats before cleanup: {stats_before}")

        # Run cleanup
        logger.info("Running cache cleanup...")
        result = cache_ops.cleanup_stale_cache()
        logger.info(f"‚úì Cleanup result: {result}")

        # Get stats after cleanup
        stats_after = cache_ops.get_cache_stats()
        logger.info(f"Cache stats after cleanup: {stats_after}")

        logger.info("‚úÖ TEST 4 PASSED: Cache Cleanup")
        return True

    except Exception as e:
        logger.error(f"‚ùå TEST 4 FAILED: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """Run all tests."""
    logger.info("\n" + "#"*80)
    logger.info("# CACHING IMPLEMENTATION TEST SUITE")
    logger.info("#"*80)

    results = {
        'Price Cache Operations': test_price_cache_operations(),
        'Cache Integration': test_cache_integration(),
        'LLM Tracking': test_llm_tracking(),
        'Cache Cleanup': test_cache_cleanup()
    }

    # Summary
    logger.info("\n" + "="*80)
    logger.info("TEST SUMMARY")
    logger.info("="*80)

    passed = 0
    failed = 0

    for test_name, result in results.items():
        status = "‚úÖ PASSED" if result else "‚ùå FAILED"
        logger.info(f"{test_name}: {status}")
        if result:
            passed += 1
        else:
            failed += 1

    logger.info(f"\nTotal: {passed} passed, {failed} failed out of {len(results)} tests")

    if failed == 0:
        logger.info("\nüéâ ALL TESTS PASSED! üéâ")
        logger.info("\nCaching implementation is working correctly!")
        logger.info("Expected improvements:")
        logger.info("  - 5-10x faster repeat analyses")
        logger.info("  - 80% reduction in API calls")
        logger.info("  - Full LLM audit trail available")
        return 0
    else:
        logger.error(f"\n‚ùå {failed} TEST(S) FAILED")
        return 1


if __name__ == "__main__":
    exit(main())
